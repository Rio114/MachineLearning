= 二値分類モデルの評価

== 二値分類モデル

=== 二値分類モデルとは

二値分類モデルは様々な形で構成できる。例えば、あるデータ点に対して対象となる事象発生の予測確率の大きさを示す値が得られるとき、適当な閾値で集団を分けて閾値より大きな予測値あのものは正、閾値より小さい予測値のものを負と予測することで二値分類を構成できる。また、ルールベースによりデータの持つ属性がある条件を満たすときに正と予測するとしても二値分類を構成できる。

後者はより一般的な場合であり条件ごとに混合行列を検証しなくてはならないが、前者は条件が予測値という一次元の数値で表されているので閾値を一次元的に動かすだけで連続的に評価できる。

従って、ここでは前者の評価方法を整理する。この場合、閾値を決めることで各データ点が混合行列のどこに入るかが決まる。これではPrecisionやRecallなどの指標は閾値次第で分類の結果が変わってしまうため、複数のモデルを比較できるように閾値に依存しない形でモデルの評価を行いたい。

そこでよく用いられるのがROC曲線やPR曲線、そしてそれらをそれぞれ一つの数字に落とし込んだものがAUCやAPである。なお、ROC曲線やPR曲線を作成するときには、モデルの予測値は確率のように0から1の範囲である必要はないが、以下では簡単のためモデルの予測値は0から1の範囲の予測確率を示すものとする。

=== 良いモデルとは

そもそも良いモデル、悪いモデルというのはどういうものなのだろうか。一旦はあまり深く考えずに以下のようなモデルがありそうだということが想像できる。

 * 神モデル：森羅万象を司る神様のように正例に対しては1、負例に対しては0を出力して、正解率100%を達成する、事象の発生を確実に予言できるモデル。

 * ポンコツモデル：正解ラベルがなんであっても二値分類スコアとして0から1の一様乱数を出力するモデル。予測には全く使えない。

 * 平凡モデル：正例ならば1に近い予測値、負例ならば0に近い予測値を返す。同じ予測値でも正解には正例と負例が混ざってる。神ほどはっきり予測はできていないが、ポンコツよりは有益な情報を得られる。

 * 天邪鬼モデル：正例に対しては低い予測値、負例に対しては高い予測値を返すモデル。予測を反転させれば平凡なモデルになるため、実質的に平凡モデルである。

確実ではないがそこそこな予測ができる平凡なモデルが複数あったとして、それらを比較するためにはその良し悪しを数値で表現したい。
そこでよく用いられるのがAUCという指標である。
AUCがどんなものであるかは後ほど説明するとして、忙しい人はAUCが0.8を超えると良いモデルで0.9を超えると過学習だと短絡的に判断してしまうことがよくある。
しかし、ビッグで不均衡なデータだと多様性により学習が困難となり、AUCが0.7に満たないモデルで運用していることもある。
結局、どんな精度指標でも単体では意味をなさず、必ず何を重視するのかを決めたうえで、相対比較を行っていくことが客観的な評価となる。

=== 閾値による二値分類
正例であることを予測する値、予測値を出力する二値分類モデルでは、スコアが高いほど正例である可能性が高いことが期待される。
そこでスコアの順番にデータを並べて、ある閾値によってデータを二分割してスコアが高い集団を正、低い集団を負と予測することで二値分類を行うことができる。
二値分類のトレードオフは誤検知（FP）と見逃し（FN）であることを思い出すと、この二つをプロットすることにより、二値分類モデルの性質を可視化できそうである。
慣例に従って、見逃し（FN）をRecallで評価すると誤検知は以下のように二通りの見方がある。

 * FPが負例のうちの何割か。FPRで誤検知を評価する。
 * FPが正の予測のうちの何割か。Precisionで誤検知を評価する。

前者はROC曲線といい、後者はRP曲線という。

== ROC曲線
ROC曲線では見逃しをTPR（Recall）、誤検知をFPRで評価する。
通常、負例の中から正と予測した割合（FPR）よりも正例の中から正と予測した割合（TPR）のほうが大きくなる。
もし逆にFPR>TPRあれば正の予測を負の予測だと読み替えることで負例を効率的に集めることができる。
閾値を連続的に変えていったときに、FPRとTPRの組み合わせがどのように変化していくかを視覚化したのがReceiver Operating Characteristic curve（受信者動作特性曲線)、略してROC曲線である。

=== 曲線の描き方
スコアの順番にデータ点を並べて、閾値を動かしていった時に、x軸にFPR、y軸にTPRを取ったものがROC曲線である。
次の二つの表はスコアもflgも同一であるが、閾値だけが異なるためNo.5, 6の混合行列のクラスが異なっている。
それに伴って、FPRとTPRの組み合わせも異なっている。

//table[thres06][スコア0.6以上は正予測]{
No.	Score	flg	class
------------
1	0.9	1	TP
2	0.8	1	TP
3	0.7	0	FP
4	0.6	1	TP
:	（閾値）	:	:
5	0.5	1	FN
6	0.4	0	TN
7	0.3	1	FN
8	0.2	0	TN
9	0.1	0	TN
//}

//listnum[roc06][スコア0.6以上を正予測とするときのFPR, TPR]{
FPR = FP数 / （flg=0の数） = 1 / 5 = 0.2
TPR = TP数 / （flg=1の数） = 3 / 5 = 0.6
//}

//table[thres04][スコア0.4以上は正予測]{
No.	Score	flg	class
------------
1	0.9	1	TP
2	0.8	1	TP
3	0.7	0	FP
4	0.6	1	TP
5	0.5	1	TP
6	0.4	0	FP
:	（閾値）	:	:
7	0.3	1	FN
8	0.2	0	TN
9	0.1	0	TN
//}

//listnum[roc04][スコア0.4以上を正予測とするときのFPR, TPR]{
FPR = FP数 / （flg=0の数） = 2 / 5 = 0.4
TPR = TP数 / （flg=1の数） = 4 / 5 = 0.8
//}

同様の表とFPRとTPRの組み合わせをスコアの値の数だけ作ることができる。
上の表の例でいうと、スコアは0.9から0.1まで0.1刻みで9個の値があるので、1.0以上、0.9以上、、、0.1以上というように10個の閾値を設定できる。
従って、上記の例ではROC曲線は10個の点を結んだものになる。

//image[roc][ROC曲線][scale=0.7]{
//}

=== 曲線の特徴

でたらめなスコアを出力するポンコツモデルであれば、正例と負例でスコアの分布が同じものになり、どんな閾値でもFPRもTPRも等しくなる。
つまり、ポンコツモデルのROCは斜め45度の線になる。

正例を全て捕捉するために多くのデータを正であると予測すると、その正予測の中には多くの負例が混入することになり、FPRが高くなる
また、負例をすべて間違いなく負と予測したければ、データ全体を負と予測すれば達成できるが、その時は正例を全く捕捉できない。

次の二つの図では、厳選して正予測を行ったときと多めに正予測をした時の混合行列をイメージしたものである。マスの大きさが分類されたデータの数を示している。
正例と負例では集団が異なるので同じ閾値を用いても正と予測される割合は異なるが、二値分類モデルが天邪鬼でない限り、正例の内の正予測の割合が大きくなる。つまり、FPRよりTPRが大きい。
また、多めに正の予測を行うと、相応に負例を正と予測する割合も増える。
以上のことから、ROC曲線は斜め45度線より上に弓なりの形状となる。

//image[conf_mat_unbalance][【上段】厳選して正予測を行ったときの混合行列、【下段】多めに正予測を行ったときの混合行列）][scale=0.5]{
//}

なお、スコアは順番に並べることができれば確率値のように0から1の範囲である必要はない。上記の表の例でいうと、スコアを一律2倍にしてもデータの順番は変わらない。さらに言えば、非線形な変換であっても順序が保たれていれば、ROC曲線は不変である。逆に言うと、ROC曲線はスコアの順序が正例負例の割合を反映しているかを読み取ることができるものの、絶対値に関する情報を読み取ることができない。

=== 使用例
冒頭で述べた通り、ROC曲線は見逃しに対する誤検知を負例の正予測で評価している。

そのため、
正負ともに分類の数を元に議論をする場合には便利である。例えば、ある病気の検査において正しく陽性を言い当てることの利益に加え、誤検知や見逃しによる損失を同時に考慮したい場合に
何らかの損失が考えられる場合、偽陽性の数の増減により全体の損失がどのように変動するかを議論できる。

=== AUC
ROCの下側面積Area Under Curveを略してAUCと言い、モデルの良し悪しを一つの数値で表すことができる。これはTPR、すなわちRecallの平均と見ることができるので、モデルによる正例の捕捉をどれだけ期待できるかを示している。

ポンコツなモデルだとAUCは0.5になる。一方、一つの閾値で正負を分けることができればAUCは1になる。

AUCが同じなら左のほうの立ち上がりが良いほうが良いモデルである。なぜならその場合、FPRをあまり上げずにTPRを上げることができているからである。

== Recall-Precision曲線
見逃しをRecallで評価し、誤検知を正予測に対する割合Precisionで評価したいときにはRecall-Precision曲線を用いる。

つまり、正例を多く捕捉しつつ、正の予測ではなるべく多くを言い当てたい。ROC曲線とのニュアンスの違いが難しいが、こちらでは負例の数についてはあまり興味がない状況を念頭に置いている。そういった目的のために、二値分類スコアの閾値に対して得られる様々なRecallとPrecisionの組み合わせをプロットしたのがRecall-Precision曲線、略してRP曲線である。これにより、正例の捕捉割合を上げることで 正の予測の正しさがどれだけ下がるかのトレードオフを視覚化できる。

=== 曲線の描き方

スコアの順番にデータ点を並べて、閾値を動かしていった時に、x軸にRecall、y軸にPrecisionを取ったものがRecall-Precision曲線である。

//listnum[rp06][TPR, FPRの計算例]{
Recall = TP数 / （flg=1の数） = 3 / 5 = 0.6
Precision = TP数 / （TP+FPの数） = 3 / 4 = 0.75
//}

//listnum[rp04][TPR, FPRの計算例]{
Recall = TP数 / （flg=1の数） = 4 / 5 = 0.8
Precision = TP数 / （TP+FPの数） = 4 / 6 = 0.66
//}

=== AP（RP曲線のAUC）
RP曲線でもAUCを考えることができる。これはROCのAUCと区別してAverage Precision、略してAPという。名前の通り、APはモデルによる正例の言い当て率がどれくらい期待できるかということを示している。

ポンコツなモデルでは、モデルで予測を行う前の集団の正例の割合となる。一方、一つの閾値で正負を分けることができればAPは1になる。

=== 曲線の特徴
正例を多く捉えるために閾値を緩くすることで、負例を誤って正としてしまう（FP）数が増えるので通常右肩下がりの曲線となる。ただし、特に不均衡データのモデル学習が足りない場合、左のほうでPrecisionの分母となる正の予測数（TP+FP）が少ないため変動が大きい。

*正例について捕捉できた割合と言い当てた割合、四角を十字で分けた図*

RP曲線もROC曲線と同様に、スコアは順番に並べることができれば確率値のように0から1の範囲である必要はない。逆に言うと、RP曲線はスコアの順序性を反映しているものの、絶対値に関する情報を読み取ることができない。

==== RP曲線は不均衡データが一目瞭然
正の言い当てを重視する場合にはROCよりも便利である。例えば、正例を半分集める（Recallが0.5）ためには30％の言い当て率（Precisionが0.3）で我慢しなくてはならないといった具合である。この際、負例には興味がなく、スコアの最大値をとるようなバンディット問題では有用だと言える。


また、AUCはスコアの順序性しか見ていないため、何らかのシステムでスコアの数値を下流で処理に用いる場合には注意が必要がある。例えば、二つのモデルのスコアを比較して高いほうを採用するといった文脈付きバンディット問題を考える場合にはスコアの絶対値が比較可能かを検証しておくべきである。
