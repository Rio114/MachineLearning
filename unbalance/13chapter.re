= 二値分類スコアのキャリブレーション

前章の二値分類モデルの評価では、ROCでもRPでもスコアの順序が正例である可能性の高さの順序になっているかを確認するものであることを見た。
しかし、予測値がそのまま確率値になっていると何かと都合が良い。
例えば、あるユーザに対して二つのうち一つのサービスを訴求するとき、それぞれのサービス加入予測モデルを使ってスコアの高い方を訴求するということを考える。
このとき、両方のモデルから得られるスコアがそのまま予測値を示していると、単純にスコアの高い方を訴求すればそのまま加入確率の高いサービスを訴求することになる。
しかし、スコアのキャリブレーションが不十分で予測値の大小と加入確率の大小で逆転している場合、予測値の大きい方を選択すると実は加入確率が低い方を選択してしまっていたということが起こりうる。
サービスごとの学習度合いが異なるとこういったことが起こりやすいためスコアのキャリブレーションが必要になる。

== モデル
神モデルとポンコツモデル
 # * ベルヌーイ試行モデル：予測値がそのまま予測確率になっているモデル。つまり、同等な予測値の集団を集めてきた場合、正例の数がスコアをそのまま母数ｐとするベルヌーイ試行に従うというモデルである。この場合、スコアとスコアで層別化した正例率をプロットすると直線に乗り、相関係数は１になる。


== スコア散布図
ROCやRPはスコア順にデータ点を並べて集計することにより作った。つまり、スコアの絶対値は評価されていない。しかし、二値分類スコアそのものがどの程度、正例を予測しているかという、スコアの絶対値に興味がある場合がある。それぞれのデータ点は0か1のいずれかであり、x軸にスコア、y軸に正負をそのままプロットしても解釈が難しい。そこで、スコアの近いものをまとめて（例えば、0.01刻みで層別化する）、正負の0－1フラグの平均を代表させることで、スコアが二項分布の確率母数を表すと解釈する。そのようにスコアと予測確率を一対一に対応させることができれば、例えば異なるモデル間でスコアを比較し、その大小で判断を行うようなバンディット問題に応用できる。これを散布図としてプロットすれば視覚的にモデルの良し悪しを検証できる。

*書き方の図示*

=== logloss
異なるモデル間でのスコアの比較を考えると、同じスコアは同じ予測確率を示すことが前提となる。それは散布図では同一のグラフに乗ることを意味しており、特に数値の解釈性の観点から、斜め45度の直線に乗るようなスコア＝予測確率となることが望ましい。

都合の良いことに、説明変数が同じでスコアが同一になると考えられる集団があったとすると、二項分布の最尤推定を行うことでスコア＝予測確率となる。これは形式的にはloglossを最小化することと同等である。二値分類予測の損失関数としてloglossが用いられることが多いのはこういった理由がある。

*y=0, 1のそれぞれに点が集まっている図*

*y=0, 1のそれぞれに点が集まっている図、あるスコアで層別したときに割合を計算する図*

== 評価例
冒頭で挙げたモデルに対して、それぞれのモデル評価がどのようになるかを見比べたい。

||視覚化|指標|
|---|---|---|
|Recall|ROC曲線|AUC|
|Precision|RP曲線|AP|
|絶対値|散布図|logloss|
